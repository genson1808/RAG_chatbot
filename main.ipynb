{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from qdrant_client import QdrantClient\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_compressors import JinaRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import FastEmbedEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from src.base.llm import SingletonChatLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph\n",
    "%pip install jina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "os.environ[\"JINA_API_KEY\"] = os.getenv(\"JINA_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(MessagesState):\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "chat_model = SingletonChatLLM(llm_name='CHAT_GROQ').get_llm()\n",
    "\n",
    "# Định nghĩa các agent cần thiết\n",
    "members = [\"product1_assistant\", \"product2_assistant\", \"customer_care\"]\n",
    "\n",
    "options = members + [\"FINISH\"]\n",
    "\n",
    "class Router(TypedDict):\n",
    "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "\n",
    "    next: Literal[*options]\n",
    "\n",
    "system_prompt = (\n",
    "    # \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    # f\" following workers: {members}. Given the following user request,\"\n",
    "    # \" respond with the worker to act next. Each worker will perform a\"\n",
    "    # \" task and respond with their results and status. When finished,\"\n",
    "    # \" respond with FINISH.\"\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    f\" following workers: {members}. Based on the user request:\"\n",
    "    \"\\n- If the request contains 'lt4670', route to 'product1_assistant'.\"\n",
    "    \"\\n- If the request contains 'bc289', route to 'product2_assistant'.\"\n",
    "    \"\\n- Otherwise, route to 'customer_care'.\"\n",
    "    \"\\nEach worker will perform a task and respond with their results and status.\"\n",
    "    \" When finished, respond with 'FINISH'.\"\n",
    ")\n",
    "\n",
    "# Define the supervisor node and agent states\n",
    "def supervisor_node(state: AgentState) -> AgentState:\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    response = chat_model.with_structured_output(Router).invoke(messages)\n",
    "    next_ = response[\"next\"]\n",
    "    if next_ == \"FINISH\":\n",
    "        next_ = END\n",
    "    return {\"next\": next_}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"You are a helpful assistant, You must use japanese to answer the question, conversing with a user about the subjects contained in a set of documents.\n",
    "Use the information from the DOCUMENTS section to provide accurate answers. If unsure or if the answer\n",
    "isn't found in the DOCUMENTS section, simply state that you don't know the answer.\n",
    "\n",
    "Documents:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "client = QdrantClient(api_key=QDRANT_API_KEY, url=QDRANT_URL,)\n",
    "\n",
    "from langchain_community.document_compressors import JinaRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "os.environ[\"JINA_API_KEY\"] = os.getenv(\"JINA_API_KEY\")\n",
    "def retrieval_qa_chain(llm, prompt, vectorstore):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 2})\n",
    "    compressor = JinaRerank()\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=compression_retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={'prompt': prompt}\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "def qa_bot():\n",
    "    embeddings = FastEmbedEmbeddings()\n",
    "    vectorstore = Qdrant(client=client, embeddings=embeddings, collection_name=\"rag2\")\n",
    "    llm = chat_model\n",
    "    qa_prompt=set_custom_prompt()\n",
    "    qa = retrieval_qa_chain(llm, qa_prompt, vectorstore)\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 54899.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LT4670 is a 1U full rack-size sync generator that outputs analog video sync signals and audio word clock.  It supports various output formats including PTP, GNSS, IP, SDI (4K/HD/3G/SD), AES/EBU, LTC, and analog sync signals. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RAG Chain\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUESTION: {question} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    Answer:\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "vectorstore = Qdrant(client=client, embeddings=embeddings, collection_name=\"rag2\")\n",
    "llm = chat_model\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 2})\n",
    "compressor = JinaRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "rag_prompt_chain = rag_prompt | llm | StrOutputParser()\n",
    "\n",
    "QUESTION = \"\"\"LT4670?\"\"\"\n",
    "CONTEXT = compression_retriever.invoke(QUESTION)\n",
    "\n",
    "result = rag_prompt_chain.invoke({\"question\": QUESTION, \"context\":CONTEXT})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 23669.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'how to setup HDR display?', 'result': 'HDR表示の設定方法については、以下の手順で操作できます。\\n\\n1. PICボタンを押します。\\n2. F2 (CINELITE/HDR) を選択します。\\n3. F5 (HDR ZONE) を選択し、表示をONまたはOFFに切り替えます。 \\n\\n\\n\\n', 'source_documents': [Document(metadata={'source': 'data2/output.md', '_id': '56d3049c-c375-4f8d-9fc8-ddffaf27c384', '_collection_name': 'rag2', 'relevance_score': 0.40686196088790894}, page_content='具体的な操作手順は以下の通りです：\\n\\nPICボタンを押す。\\n\\nF2 (CINELITE/HDR)を選択。\\n\\nF5 (HDR ZONE)を選択し、表示をOFFまたはONに切り替える。\\n\\nこの操作により、HDRの測定や評価がより直感的に行えるようになります。シネゾーン表示は、HDRコンテンツの制作や評価において、視覚的な情報を提供するための重要なツールです。\\n\\n図には、シネゾーン表示のインターフェースや設定が示されており、具体的な操作や表示内容が視覚的に理解できるようになっています。 このページでは、HDR測定における基準レベルの設定方法について説明されています。HDRタブのVARIABLEがONの状態で、基準レベルを可変にすることができ、これはSDR表示とHDR表示の境界線を示します。\\n\\n具体的な操作手順は以下の通りです：\\n\\nPIC ➡ F2 (CINELITE/HDR) ➡ F2 (CINEZONE SETUP) ➡ F4 (REF [%]): ここで基準レベルを0.0から100.0の範囲で設定します。\\n\\n画面左上には、F2 (UPPER [%])、F3 (LOWER [%])、F4 (REF [%])で設定した値のHDR換算値が表示されます。\\n\\n表示される情報には、以下のようなデータが含まれています： - Upper: 100.0 cd/m² - Lower: 50.7 cd/m² - REF: 0.0 cd/m²\\n\\nまた、シネゾーン表示に関する図も含まれており、設定された値がどのように表示されるかを視覚的に示しています。\\n\\nこの情報は、HDR測定を行う際に重要な基準となるため、正確に設定することが求められます。\\n\\nMAX FALL/CLL 表示\\n\\nシネライト表示では、MAX FALL (Maximum Frame Average Light Level) および MAX CLL (Maximum Content Light Level) を表示できます。 - MAX FALL はフレームごとの平均最大輝度を示し、 - MAX CLL はコンテンツの明るさの最大値を示します。\\n\\nMAX FALL/CLL の表示方法\\n\\n表示をONにする:\\n\\n操作手順: PIC ➡ F2 (CINELITE/HDR) ➡ F3 (MAX FALL/CLL) ➡ F1 (MAX FALL/CLL DISPLAY): OFF / ON\\n\\n測定期間の設定:\\n\\n測定開始時に START、終了時に STOP を選択します。\\n\\n操作手順: PIC ➡ F2 (CINELITE/HDR) ➡ F3 (MAX FALL/CLL) ➡ F2 (MEASURE): START / STOP\\n\\n測定のクリア:\\n\\n測定値が 0% に戻り、F2 (MEASURE) が STOP になります。\\n\\n操作手順: PIC ➡ F2 (CINELITE/HDR) ➡ F3 (MAX FALL/CLL) ➡ F3 (CLEAR)\\n\\n表示例\\n\\nMAX CLL: 1866.45 cd/m²\\n\\nMAX FALL: 16.28 cd/m²\\n\\n解像度: 3840x2160 / 59.94P YCbCr(422)\\n\\nその他の情報: SDI A-D, TIME: 16.01.59\\n\\nこの情報は、HDRコンテンツの輝度特性を理解するために重要です。\\n\\n用語集\\n\\nこのセクションでは、HDRに関連する重要な用語について説明します。'), Document(metadata={'source': 'data2/output.md', '_id': '875919ac-495f-4537-b053-158f76ff6135', '_collection_name': 'rag2', 'relevance_score': 0.25091278553009033}, page_content='HD: 1.485Gbps、1.485/1.001Gbps\\n\\nSD: 270Mbps\\n\\nタイミング可変\\n\\n可変範囲: フレーム全範囲\\n\\n可変単位\\n\\nV: ライン単位\\n\\nH: クロック単位 (148.5MHz、148.5/1.001MHz、74.25MHz、74.25/1.001MHz、27MHz)\\n\\nタイミング基準の選択\\n\\nSD、HD のみ: 3G と 12G は SERIAL のみ\\n\\nSERIAL: 信号規格で定義されたタイミングで出力\\n\\nLEGACY: 従来の当社信号発生器と同じタイミングで出力\\n\\nテストパターン\\n\\n12G、3G、HD\\n\\n100%カラーバー、75%カラーバー、マルチフォーマットカラーバー (ARIB STD-B28、パターン 2 の部分を 100%白/75%白/+I から選択可)、チェックフィールド、フラットフィールド白 100%、白 50%、黒 0%、赤 100%、緑 100%、青 100%\\n\\nSD\\n\\n525/59.94I: 100%カラーバー、75%カラーバー、SMPTE カラーバー、チェックフィールド、フラットフィールド白 100%、白 50%、黒 0%、赤 100%、緑 100%、青 100%\\n\\n625/50I: EBU カラーバー、BBC カラーバー、チェックフィールド、フラットフィールド白 100%、白 50%、黒 0%、赤 100%、緑 100%、青 100%\\n\\n4K 追加パターン (将来対応)\\n\\nUHDColorBar: ARIB STD-B66 UHDTV MULTIFORMAT COLOR BAR\\n\\nHLGCB: ARIB STD-B72 Color Bar Test Pattern for HLG HDR-TV System (勧告 ITU-R BT.2111 HLG)\\n\\nSlog3_LiveHDR_narrow_V11: S-Log3 (Live HDR) Ver.1.11 narrow range scale\\n\\nユーザーパターン表示\\n\\nSD、HD、4K 各 INT 1 - 4 から 1 つを選択\\n\\nファイル形式: 24 ビットフルカラービットマップ形式 (.bmp)、24/48 ビット TIFF 形式 (.tif) 非圧縮のみ\\n\\n自動切り換え機能\\n\\n選択可能なカラーバーパターンを自動で切り換え\\n\\n切り換え時間: 1 – 255秒\\n\\nこの仕様は、SYNC GENERATOR LT4670のSDIビデオ出力に関する詳細な情報を提供しています。各ビットレートやタイミングの設定、テストパターンの種類などが明記されており、ユーザーが必要な設定を行う際の参考になります。\\n\\n3 仕様\\n\\nパターンスクロール\\n\\n方向: 8方向 (上下左右とその組み合わせ)\\n\\nスピード範囲と単位:\\n\\nインターレース: フィールド単位\\n\\nV: ±256ライン (1ライン単位)\\n\\nH: ±256ドット (2ドット単位)\\n\\nプログレッシブ: フレーム単位\\n\\nV: ±256ライン (1または2ライン単位)\\n\\nH: ±256ドット (2または4ドット単位)\\n\\nチェックフィールドパターン選択時は無効です。\\n\\nセーフティエリアマーカー\\n\\n12G、3G、HD:\\n\\nアクションセーフティエリア (90%)\\n\\nタイトルセーフティエリア (80%)\\n\\n4:3 アスペクト (個別にオンオフ可)\\n\\nSD:\\n\\nアクションセーフティエリア (90%)\\n\\nタイトルセーフティエリア (80%)\\n\\n(個別にオンオフ可)\\n\\nチェックフィールドパターン選択時は無効です。\\n\\nID キャラクター\\n\\n文字数: 最大 20文字\\n\\nサイズ: 32×32 / 64×64 / 128×128 / 256×256 dot\\n\\n輝度: 100% / 75% (背景は黒のみ)\\n\\n表示位置: 画面上任意の位置\\n\\n表示位置可変範囲:\\n\\nV: 0 - 100% (1%単位)\\n\\nH: 0 - 100% (1%単位)\\n\\n点滅表示 (*1):\\n\\nオン / オフ\\n\\nオン時間: 1 - 9秒 (1秒単位)\\n\\nオフ時間: 1 - 9秒 (1秒単位)\\n\\nスクロール機能 (*1):\\n\\n機能: IDキャラクターの背景を含めてスクロール\\n\\n方向: 2方向 (左右)\\n\\nスピード範囲と単位:\\n\\nインターレース: フィールド単位\\n\\n±256ドット (2ドット単位)\\n\\nプログレッシブ: フレーム単位\\n\\n±256ドット (2または4ドット単位)\\n\\nチェックフィールドパターン選択時は無効です。 1 点滅表示とスクロール機能は同時に設定できます。')]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HDR表示の設定方法については、以下の手順で操作できます。\\n\\n1. PICボタンを押します。\\n2. F2 (CINELITE/HDR) を選択します。\\n3. F5 (HDR ZONE) を選択し、表示をONまたはOFFに切り替えます。 \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = qa_bot()\n",
    "# response = qa({\"query\": \"how to setup HDR display?\"})\n",
    "response = qa(\"how to setup HDR display?\")\n",
    "print(response)\n",
    "response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the QA agent nodes\n",
    "def qa_product1_agent_node(state: AgentState) -> AgentState:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "\n",
    "    result = qa({\"query\": user_input})\n",
    "\n",
    "    response = result['result']\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_product2_agent_node(state: AgentState) -> AgentState:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "\n",
    "    result = qa({\"query\": user_input})\n",
    "\n",
    "    response = result['result']\n",
    "\n",
    "    return {\n",
    "        \"messages\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_customer_care_agent_node(state: AgentState) -> AgentState:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "\n",
    "    result = qa({\"query\": user_input})\n",
    "\n",
    "    response = result['result']\n",
    "\n",
    "    return {\n",
    "        \"messages\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor(state: AgentState):\n",
    "    \"\"\"\n",
    "    Supervises the workflow and decides which agent to call based on the question.\n",
    "    \"\"\"\n",
    "    # Simple logic to route based on product name in the question\n",
    "    user_query = state[\"messages\"][-1].content.lower()\n",
    "    print(user_query)\n",
    "\n",
    "    if \"lt4670\" in user_query:\n",
    "        return {\"next\": \"qa_product1_agent_node\"}\n",
    "    elif \"lxm3\" in user_query:\n",
    "        return {\"next\": \"qa_product2_agent_node\"}\n",
    "    else:\n",
    "        return {\"next\": \"qa_customer_care_agent_node\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the agent state graph and add conditional edges\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"product1_assistant\", qa_product1_agent_node)\n",
    "builder.add_node(\"product2_assistant\", qa_product2_agent_node)\n",
    "builder.add_node(\"customer_care\", qa_customer_care_agent_node)\n",
    "# # === Graph Setup ===\n",
    "# builder = StateGraph(AgentState)\n",
    "# builder.add_node(supervisor)\n",
    "# builder.add_node(qa_product1_agent_node)\n",
    "# builder.add_node(qa_product2_agent_node)\n",
    "# builder.add_node(qa_customer_care_agent_node)\n",
    "\n",
    "# builder.add_edge(START, \"supervisor\")\n",
    "# builder.add_conditional_edges(\"supervisor\", lambda state: state[\"next\"])\n",
    "builder.add_edge(\"product1_assistant\", \"__end__\")\n",
    "builder.add_edge(\"product2_assistant\", \"__end__\")\n",
    "builder.add_edge(\"customer_care\", \"__end__\")\n",
    "\n",
    "# for member in members:\n",
    "#     builder.add_edge(member, \"supervisor\")\n",
    "\n",
    "builder.add_conditional_edges(\"supervisor\", lambda state: state[\"next\"])\n",
    "\n",
    "# Compile the graph and execute it\n",
    "graph = builder.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'product1_assistant'}}\n",
      "----\n",
      "{'product1_assistant': {'messages': ['LT4670のHDR表示の設定方法について説明します。\\n\\n1. **PICボタンを押します。**\\n2. **F2 (CINELITE/HDR) を選択します。**\\n3. **F2 (CINEZONE SETUP) を選択します。**\\n4. **F4 (REF [%]) を選択し、基準レベルを0.0から100.0の範囲で設定します。**\\n\\nこれでHDRの測定や評価がより直感的に行えるようになります。 \\n\\n\\n']}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"lt4670: How to configure the HDR display?\")]}\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n",
    "\n",
    "# for s in graph.stream(\n",
    "#     {\"messages\": [(\"user\", \"how are you\")]}\n",
    "# ):\n",
    "#     print(s)\n",
    "#     print(\"----\")\n",
    "\n",
    "# res = graph.invoke({\"messages\": [(\"user\", \"lt4670: How to configure the HDR display?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.40.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in ./.venv/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in ./.venv/lib/python3.12/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in ./.venv/lib/python3.12/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in ./.venv/lib/python3.12/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in ./.venv/lib/python3.12/site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in ./.venv/lib/python3.12/site-packages (from streamlit) (5.28.3)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Using cached pyarrow-18.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./.venv/lib/python3.12/site-packages (from streamlit) (2.32.3)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from streamlit) (8.5.0)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in ./.venv/lib/python3.12/site-packages (from streamlit) (4.12.2)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in ./.venv/lib/python3.12/site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.5.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.21.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m333.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m280.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Using cached pyarrow-18.0.0-cp312-cp312-macosx_12_0_arm64.whl (29.5 MB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m344.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hUsing cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading narwhals-1.14.1-py3-none-any.whl (220 kB)\n",
      "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.21.0-cp312-cp312-macosx_11_0_arm64.whl (321 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: toml, smmap, rpds-py, pyarrow, narwhals, mdurl, cachetools, blinker, referencing, pydeck, markdown-it-py, gitdb, rich, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed altair-5.4.1 blinker-1.9.0 cachetools-5.5.0 gitdb-4.0.11 gitpython-3.1.43 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 markdown-it-py-3.0.0 mdurl-0.1.2 narwhals-1.14.1 pyarrow-18.0.0 pydeck-0.9.1 referencing-0.35.1 rich-13.9.4 rpds-py-0.21.0 smmap-5.0.1 streamlit-1.40.1 toml-0.10.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 01:19:41.395 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.395 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2024-11-21 01:19:41.395 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.396 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.434 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.435 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.484 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/genson1808/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-11-21 01:19:41.485 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.485 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.485 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-21 01:19:41.487 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# Constants\n",
    "height = 600\n",
    "title = \"Multi-Agent Software Team (LangGraph)\"\n",
    "icon = \":robot:\"\n",
    "\n",
    "def generate_message(user_input):\n",
    "    response = graph.invoke({\"messages\": [HumanMessage(content=user_input)]})\n",
    "    ai_messages = [msg for msg in response[\"messages\"] if isinstance(msg, AIMessage)]\n",
    "\n",
    "    st.session_state.conversation.append({\n",
    "        \"user\": user_input,\n",
    "        \"assistant\": ai_messages[-1].content,\n",
    "    })\n",
    "\n",
    "    # Iterate over the conversation history\n",
    "    for entry in st.session_state.conversation:\n",
    "        messages.chat_message(\"user\", avatar=\"img/user.png\").write(entry['user'])\n",
    "        messages.chat_message(\"ai\", avatar=\"img/summary.png\" ).write(\"**assistant:** \\n\" + entry['assistant'])\n",
    "\n",
    "# Session: Initialize conversation history\n",
    "if \"conversation\" not in st.session_state:\n",
    "    st.session_state.conversation = []\n",
    "\n",
    "# Set page title and icon\n",
    "st.set_page_config(page_title=title, page_icon=icon)\n",
    "st.header(title)\n",
    "\n",
    "# Create a container for the chat messages\n",
    "messages = st.container(border=True, height=height)\n",
    "\n",
    "# Chatbot UI\n",
    "if prompt := st.chat_input(\"Enter your question...\", key=\"prompt\"):\n",
    "    generate_message(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([{'next': 'customer_care'}])\n",
      "dict_values([{'messages': [HumanMessage(content=['こんにちは！ \\n'], additional_kwargs={}, response_metadata={}, name='customer_care', id='ab0b82e7-7ca1-4a6f-b060-67d3c45ccd2d')]}])\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_call\": {\\n\\t\\t\"id\": \"pending\",\\n\\t\\t\"type\": \"function\",\\n\\t\\t\"function\": {\\n\\t\\t\\t\"name\": \"Router\"\\n\\t\\t},\\n\\t\\t\"parameters\": {\\n\\t\\t\\t\"next\": \"pending\"\\n\\t\\t}\\n\\t}\\n}\\n</tool-use> \\n'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGood Bye\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;66;03m# Chuyển dict_values thành danh sách\u001b[39;00m\n\u001b[1;32m      9\u001b[0m  \n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;66;03m# for message in messages:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;66;03m#     print(message.content)\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1646\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1653\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:104\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    102\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, writer)\u001b[0m\n\u001b[1;32m     38\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[100], line 41\u001b[0m, in \u001b[0;36msupervisor_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msupervisor_node\u001b[39m(state: AgentState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentState:\n\u001b[1;32m     40\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt}] \u001b[38;5;241m+\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRouter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     next_ \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINISH\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_groq/chat_models.py:474\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    470\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    473\u001b[0m }\n\u001b[0;32m--> 474\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/groq/resources/chat/completions.py:287\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/groq/_base_client.py:1244\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1232\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1241\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1242\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1243\u001b[0m     )\n\u001b[0;32m-> 1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/groq/_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/groq/_base_client.py:1039\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1038\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1039\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1042\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1043\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1048\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_call\": {\\n\\t\\t\"id\": \"pending\",\\n\\t\\t\"type\": \"function\",\\n\\t\\t\"function\": {\\n\\t\\t\\t\"name\": \"Router\"\\n\\t\\t},\\n\\t\\t\"parameters\": {\\n\\t\\t\\t\"next\": \"pending\"\\n\\t\\t}\\n\\t}\\n}\\n</tool-use> \\n'}}"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  user_input=input(\"User: \")\n",
    "  if user_input.lower() in [\"quit\",\"q\"]:\n",
    "    print(\"Good Bye\")\n",
    "    break\n",
    "  for event in graph.stream({'messages':(\"user\",user_input)}):\n",
    "    print(event.values())\n",
    "    # Chuyển dict_values thành danh sách\n",
    "\n",
    "    # # Truy cập tin nhắn\n",
    "    # messages = response_list[0]['messages']\n",
    "\n",
    "    # # Lấy nội dung từ HumanMessage\n",
    "    # for message in messages:\n",
    "    #     print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_call\": {\\n\\t\\t\"id\": \"pending\",\\n\\t\\t\"type\": \"function\",\\n\\t\\t\"function\": {\\n\\t\\t\\t\"name\": \"Router\"\\n\\t\\t},\\n\\t\\t\"parameters\": {\\n\\t\\t\\t\"next\": \"pending\"\\n\\t\\t}\\n\\t}\\n}\\n</tool-use> \\n'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res1 \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhow are you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1918\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1917\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1918\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1646\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1653\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:104\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    102\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, writer)\u001b[0m\n\u001b[1;32m     38\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[46], line 41\u001b[0m, in \u001b[0;36msupervisor_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msupervisor_node\u001b[39m(state: AgentState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentState:\n\u001b[1;32m     40\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt}] \u001b[38;5;241m+\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRouter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     next_ \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINISH\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/langchain_groq/chat_models.py:474\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    470\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    473\u001b[0m }\n\u001b[0;32m--> 474\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/groq/resources/chat/completions.py:287\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/groq/_base_client.py:1244\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1232\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1241\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1242\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1243\u001b[0m     )\n\u001b[0;32m-> 1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/groq/_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/ai/rag_advanced/RAG-chat-with-documents/.venv/lib/python3.12/site-packages/groq/_base_client.py:1039\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1038\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1039\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1042\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1043\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1048\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_call\": {\\n\\t\\t\"id\": \"pending\",\\n\\t\\t\"type\": \"function\",\\n\\t\\t\"function\": {\\n\\t\\t\\t\"name\": \"Router\"\\n\\t\\t},\\n\\t\\t\"parameters\": {\\n\\t\\t\\t\"next\": \"pending\"\\n\\t\\t}\\n\\t}\\n}\\n</tool-use> \\n'}}"
     ]
    }
   ],
   "source": [
    "res1 = graph.invoke({\"messages\": [(\"user\", \"how are you\")]})\n",
    "print(res['messages'][-1].content)\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = graph.invoke({\"messages\": [(\"user\", \"now, what time is it?\")]})\n",
    "print(res['messages'][-1].content)\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client = QdrantClient(api_key=QDRANT_API_KEY, url=QDRANT_URL)\n",
    "\n",
    "# Initialize chat model (Groq)\n",
    "chat_model = SingletonChatLLM(llm_name='CHAT_GROQ').get_llm()\n",
    "\n",
    "# Define a custom prompt template for the QA retrieval process\n",
    "custom_prompt_template = \"\"\"You are a helpful assistant. You must use Japanese to answer the question, conversing with a user about the subjects contained in a set of documents.\n",
    "Use the information from the DOCUMENTS section to provide accurate answers. If unsure or if the answer isn't found in the DOCUMENTS section, simply state that you don't know the answer.\n",
    "\n",
    "Documents:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    \"\"\"Setup prompt template for each QA agent.\"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "# Function to create a QA chain using retrieval from Qdrant\n",
    "def retrieval_qa_chain(llm, prompt, vectorstore):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 2})\n",
    "    compressor = JinaRerank()\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=compression_retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={'prompt': prompt}\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "\n",
    "# Query Qdrant collection\n",
    "def query_qdrant_collection(collection_name: str, query_vector: list):\n",
    "    \"\"\"Query data from Qdrant based on the search vector.\"\"\"\n",
    "    result = client.search(collection_name=collection_name, query_vector=query_vector, limit=5)\n",
    "    return result\n",
    "\n",
    "# Define the QA agent nodes\n",
    "def qa_product1_agent_node(state: AgentState) -> AgentState:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    prompt = set_custom_prompt()\n",
    "\n",
    "    # Generate query vector for search (you should replace this with actual vector generation logic)\n",
    "    query_vector = [0.1, 0.2, 0.3]\n",
    "    result = query_qdrant_collection(\"product1_collection\", query_vector)\n",
    "\n",
    "    qa_chain = retrieval_qa_chain(chat_model, prompt, Qdrant(client=client, embeddings=embeddings, collection_name=\"product1_collection\"))\n",
    "    response = qa_chain.run(input={\"question\": user_input, \"context\": result})\n",
    "\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=f\"Câu trả lời cho sản phẩm 1: {response}\", name=\"product1\")]\n",
    "    }\n",
    "\n",
    "def qa_product2_agent_node(state: AgentState) -> AgentState:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    prompt = set_custom_prompt()\n",
    "\n",
    "    query_vector = [0.4, 0.5, 0.6]\n",
    "    result = query_qdrant_collection(\"product2_collection\", query_vector)\n",
    "\n",
    "    qa_chain = retrieval_qa_chain(chat_model, prompt, Qdrant(client=client, embeddings=embeddings, collection_name=\"product2_collection\"))\n",
    "    response = qa_chain.run(input={\"question\": user_input, \"context\": result})\n",
    "\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=f\"Câu trả lời cho sản phẩm 2: {response}\", name=\"product2\")]\n",
    "    }\n",
    "\n",
    "def qa_customer_care_agent_node(state: AgentState) -> AgentState:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    prompt = set_custom_prompt()\n",
    "\n",
    "    query_vector = [0.7, 0.8, 0.9]\n",
    "    result = query_qdrant_collection(\"customer_care_collection\", query_vector)\n",
    "\n",
    "    qa_chain = retrieval_qa_chain(chat_model, prompt, Qdrant(client=client, embeddings=embeddings, collection_name=\"customer_care_collection\"))\n",
    "    response = qa_chain.run(input={\"question\": user_input, \"context\": result})\n",
    "\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=f\"Câu trả lời chăm sóc khách hàng: {response}\", name=\"customer_care\")]\n",
    "    }\n",
    "\n",
    "# Define the supervisor node and agent states\n",
    "def supervisor_node(state: AgentState) -> AgentState:\n",
    "    system_prompt = (\n",
    "        \"You are a supervisor tasked with managing conversations for the following workers: {members}. \"\n",
    "        \"Given the following user request, respond with the worker to act next. Each worker will perform a task \"\n",
    "        \"and respond with their results and status. When finished, respond with FINISH.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    response = chat_model.with_structured_output(Router).invoke(messages)\n",
    "    next_ = response[\"next\"]\n",
    "    if next_ == \"FINISH\":\n",
    "        next_ = END\n",
    "    return {\"next\": next_}\n",
    "\n",
    "# Setup the agent state graph and add conditional edges\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"product1\", qa_product1_agent_node)\n",
    "builder.add_node(\"product2\", qa_product2_agent_node)\n",
    "builder.add_node(\"customer_care\", qa_customer_care_agent_node)\n",
    "\n",
    "for member in members:\n",
    "    builder.add_edge(member, \"supervisor\")\n",
    "\n",
    "builder.add_conditional_edges(\"supervisor\", lambda state: state[\"next\"])\n",
    "\n",
    "# Compile the graph and execute it\n",
    "graph = builder.compile()\n",
    "\n",
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Cấu hình sản phẩm 1 như thế nào?\")]}, subgraphs=True\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
